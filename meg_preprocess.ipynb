{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## meg preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### setup\n",
    "1. mne install\n",
    "```\n",
    "conda install --channel=conda-forge --name=base mamba\n",
    "mamba create --override-channels --channel=conda-forge --name=mne mne\n",
    "```\n",
    "\n",
    "2. freesurfer\n",
    "- https://surfer.nmr.mgh.harvard.edu/\n",
    "\n",
    "3. afni\n",
    "- ttps://afni.nimh.nih.gov/pub/dist/doc/htmldoc/background_install/install_instructs/steps_linux_ubuntu20.html#quick-setup\n",
    "\n",
    "#### preprocess steps\n",
    "##### mne\n",
    "1. cut data\n",
    "2. denoise\n",
    "3. BEM model\n",
    "4. source localization\n",
    "##### region-based mask\n",
    "1. downsample with 3dfractionize\n",
    "2. mask with hcp atlas\n",
    "\n",
    "#### function\n",
    "1. data: T1 and raw '.fif'., best with suffix '_raw_tsss_mc.fif'\n",
    "2. ouput: '.mat' with 180 regions according to hcp glasser atlas.\n",
    "- https://afni.nimh.nih.gov/pub/dist/atlases/MNI_HCP/MNI_Glasser_HCP_2021_v1.0a/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "IPython.notebook.set_autosave_interval(5000)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 5 seconds\n"
     ]
    }
   ],
   "source": [
    "# suppress warnings\n",
    "import warnings\n",
    "import sys \n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "import numpy as np #numpy's \"nickname\" is np\n",
    "import os\n",
    "from time import time\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "import scipy\n",
    "import csv\n",
    "import shutil\n",
    "import joblib\n",
    "import datetime\n",
    "\n",
    "import mne\n",
    "import nibabel as nib\n",
    "import scipy\n",
    "\n",
    "# for mask\n",
    "from nilearn.input_data import NiftiMasker,  MultiNiftiMasker\n",
    "import nilearn as nil\n",
    "from scipy import stats\n",
    "\n",
    "# set log level\n",
    "# mne.set_log_level(\"WARNING\")\n",
    "mne.set_log_level(\"INFO\")\n",
    "\n",
    "# display the plots inline \n",
    "%matplotlib inline \n",
    "# autosave for every 5 secs\n",
    "%autosave 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### set environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dir=os.getcwd()\n",
    "data_dir=os.path.join(project_dir,\"data\")\n",
    "freesurfer_dir=os.path.join(project_dir,\"freesurfer\")\n",
    "preprocessed_dir=os.path.join(project_dir,\"preprocessed\")\n",
    "\n",
    "if not os.path.isdir(preprocessed_dir):\n",
    "    os.mkdir(preprocessed_dir)\n",
    "\n",
    "subject_name=\"sub-xst\"\n",
    "date_string=\"20231013\"\n",
    "run_name=\"run01\"\n",
    "\n",
    "preprocessed_subject_path_str = os.path.join(preprocessed_dir, subject_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### visualize raw data\n",
    "https://www.nmr.mgh.harvard.edu/mne/0.14/auto_tutorials/plot_visualize_raw.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening raw data file /home/brain/Workspace/meg_driving/meg_preprocess/data/sub-xst/20231013/run1_tsss_mc.fif...\n",
      "    Range : 18000 ... 341999 =     18.000 ...   341.999 secs\n",
      "Ready.\n",
      "Reading 0 ... 323999  =      0.000 ...   323.999 secs...\n"
     ]
    }
   ],
   "source": [
    "raw_meg_data_file_name = \"run1_tsss_mc.fif\"\n",
    "raw_meg_data_file_path_str = os.path.join(data_dir, subject_name, date_string)\n",
    "raw = mne.io.read_raw_fif(os.path.join(raw_meg_data_file_path_str, raw_meg_data_file_name), preload=True, allow_maxshield=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw.plot(block=True)\n",
    "\n",
    "# manually find start and end square wave singal\n",
    "start_time = 16 # second\n",
    "end_time = 318"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening raw data file /home/brain/Workspace/meg_driving/meg_preprocess/data/sub-xst/20231013/run1_tsss_mc.fif...\n",
      "    Range : 18000 ... 341999 =     18.000 ...   341.999 secs\n",
      "Ready.\n",
      "Reading 0 ... 323999  =      0.000 ...   323.999 secs...\n",
      "Overwriting existing file.\n",
      "Writing /home/brain/Workspace/meg_driving/meg_preprocess/data/sub-xst/20231013/data_run1_tsss_mc.fif\n",
      "Closing /home/brain/Workspace/meg_driving/meg_preprocess/data/sub-xst/20231013/data_run1_tsss_mc.fif\n",
      "[done]\n"
     ]
    }
   ],
   "source": [
    "start_time = 16 # second\n",
    "\n",
    "data_length = 5 * 60 # second\n",
    "\n",
    "raw_meg_data_file_name = \"run1_tsss_mc.fif\"\n",
    "raw_meg_data_file_path_str = os.path.join(data_dir, subject_name, date_string)\n",
    "\n",
    "raw = mne.io.read_raw_fif(os.path.join(raw_meg_data_file_path_str, raw_meg_data_file_name), preload=True, allow_maxshield=True)\n",
    "raw = raw.crop(tmin=start_time, tmax=start_time + data_length)\n",
    "\n",
    "cut_meg_data_file_name = \"data_run1_tsss_mc.fif\"\n",
    "raw.save(os.path.join(raw_meg_data_file_path_str, cut_meg_data_file_name), overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cut to small data (for testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_length = 10 # second\n",
    "start_time = 16 # second\n",
    "\n",
    "raw_meg_data_file_name = \"run1_tsss_mc.fif\"\n",
    "raw_meg_data_file_path_str = os.path.join(data_dir, subject_name, date_string)\n",
    "\n",
    "raw = mne.io.read_raw_fif(os.path.join(raw_meg_data_file_path_str, raw_meg_data_file_name), preload=True, allow_maxshield=True)\n",
    "raw = raw.crop(tmin=start_time, tmax=start_time + data_length)\n",
    "\n",
    "cut_meg_data_file_name = \"cut10s_run1_tsss_mc.fif\"\n",
    "raw.save(os.path.join(raw_meg_data_file_path_str, cut_meg_data_file_name), overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### denoise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening raw data file /home/brain/Workspace/meg_driving/meg_preprocess/data/sub-xst/20231013/cut10s_run1_tsss_mc.fif...\n",
      "    Range : 34000 ... 44000 =     34.000 ...    44.000 secs\n",
      "Ready.\n",
      "Reading 0 ... 10000  =      0.000 ...    10.000 secs...\n",
      "Using EOG channels: EOG001, EOG002\n",
      "EOG channel index for this subject is: [0 1]\n",
      "Filtering the data to remove DC offset to help distinguish blinks from saccades\n",
      "Selecting channel EOG002 for blink detection\n",
      "Setting up band-pass filter from 1 - 10 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a two-pass forward and reverse, zero-phase, non-causal bandpass filter:\n",
      "- Windowed frequency-domain design (firwin2) method\n",
      "- Hann window\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 0.50 Hz (-12 dB cutoff frequency: 0.75 Hz)\n",
      "- Upper passband edge: 10.00 Hz\n",
      "- Upper transition bandwidth: 0.50 Hz (-12 dB cutoff frequency: 10.25 Hz)\n",
      "- Filter length: 10000 samples (10.000 s)\n",
      "\n",
      "Now detecting blinks and generating corresponding events\n",
      "Found 2 significant peaks\n",
      "Number of EOG events detected: 2\n",
      "Not setting metadata\n",
      "2 matching events found\n",
      "No baseline correction applied\n",
      "Using data from preloaded Raw for 2 events and 1001 original time points ...\n",
      "1 bad epochs dropped\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "Applying baseline correction (mode: mean)\n",
      "Using channel ECG003 to identify heart beats.\n",
      "Setting up band-pass filter from 8 - 16 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a two-pass forward and reverse, zero-phase, non-causal bandpass filter:\n",
      "- Windowed frequency-domain design (firwin2) method\n",
      "- Hann window\n",
      "- Lower passband edge: 8.00\n",
      "- Lower transition bandwidth: 0.50 Hz (-12 dB cutoff frequency: 7.75 Hz)\n",
      "- Upper passband edge: 16.00 Hz\n",
      "- Upper transition bandwidth: 0.50 Hz (-12 dB cutoff frequency: 16.25 Hz)\n",
      "- Filter length: 10000 samples (10.000 s)\n",
      "\n",
      "Number of ECG events detected : 10 (average pulse 59 / min.)\n",
      "Not setting metadata\n",
      "10 matching events found\n",
      "No baseline correction applied\n",
      "Using data from preloaded Raw for 10 events and 1001 original time points ...\n",
      "1 bad epochs dropped\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "Applying baseline correction (mode: mean)\n",
      "Fitting ICA to data using 306 channels (please be patient, this may take a while)\n",
      "Selecting by number: 50 components\n",
      "Fitting ICA took 11.0s.\n",
      "Using EOG channels: EOG001, EOG002\n",
      "... filtering ICA sources\n",
      "Setting up band-pass filter from 1 - 10 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a two-pass forward and reverse, zero-phase, non-causal bandpass filter:\n",
      "- Windowed frequency-domain design (firwin2) method\n",
      "- Hann window\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 0.50 Hz (-12 dB cutoff frequency: 0.75 Hz)\n",
      "- Upper passband edge: 10.00 Hz\n",
      "- Upper transition bandwidth: 0.50 Hz (-12 dB cutoff frequency: 10.25 Hz)\n",
      "- Filter length: 10000 samples (10.000 s)\n",
      "\n",
      "... filtering target\n",
      "Setting up band-pass filter from 1 - 10 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a two-pass forward and reverse, zero-phase, non-causal bandpass filter:\n",
      "- Windowed frequency-domain design (firwin2) method\n",
      "- Hann window\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 0.50 Hz (-12 dB cutoff frequency: 0.75 Hz)\n",
      "- Upper passband edge: 10.00 Hz\n",
      "- Upper transition bandwidth: 0.50 Hz (-12 dB cutoff frequency: 10.25 Hz)\n",
      "- Filter length: 10000 samples (10.000 s)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... filtering ICA sources\n",
      "Setting up band-pass filter from 1 - 10 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a two-pass forward and reverse, zero-phase, non-causal bandpass filter:\n",
      "- Windowed frequency-domain design (firwin2) method\n",
      "- Hann window\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 0.50 Hz (-12 dB cutoff frequency: 0.75 Hz)\n",
      "- Upper passband edge: 10.00 Hz\n",
      "- Upper transition bandwidth: 0.50 Hz (-12 dB cutoff frequency: 10.25 Hz)\n",
      "- Filter length: 10000 samples (10.000 s)\n",
      "\n",
      "... filtering target\n",
      "Setting up band-pass filter from 1 - 10 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a two-pass forward and reverse, zero-phase, non-causal bandpass filter:\n",
      "- Windowed frequency-domain design (firwin2) method\n",
      "- Hann window\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 0.50 Hz (-12 dB cutoff frequency: 0.75 Hz)\n",
      "- Upper passband edge: 10.00 Hz\n",
      "- Upper transition bandwidth: 0.50 Hz (-12 dB cutoff frequency: 10.25 Hz)\n",
      "- Filter length: 10000 samples (10.000 s)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using threshold: 0.16 for CTPS ECG detection\n",
      "Using channel ECG003 to identify heart beats.\n",
      "Setting up band-pass filter from 8 - 16 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a two-pass forward and reverse, zero-phase, non-causal bandpass filter:\n",
      "- Windowed frequency-domain design (firwin2) method\n",
      "- Hann window\n",
      "- Lower passband edge: 8.00\n",
      "- Lower transition bandwidth: 0.50 Hz (-12 dB cutoff frequency: 7.75 Hz)\n",
      "- Upper passband edge: 16.00 Hz\n",
      "- Upper transition bandwidth: 0.50 Hz (-12 dB cutoff frequency: 16.25 Hz)\n",
      "- Filter length: 10000 samples (10.000 s)\n",
      "\n",
      "Number of ECG events detected : 10 (average pulse 59 / min.)\n",
      "Not setting metadata\n",
      "10 matching events found\n",
      "No baseline correction applied\n",
      "Using data from preloaded Raw for 10 events and 1001 original time points ...\n",
      "1 bad epochs dropped\n",
      "Applying ICA to Raw instance\n",
      "    Transforming to ICA space (50 components)\n",
      "    Zeroing out 50 ICA components\n",
      "    Projecting back using 306 PCA components\n",
      "Overwriting existing file.\n",
      "Writing /home/brain/Workspace/meg_driving/meg_preprocess/preprocessed/sub-xst/sensor_prep/preprocessed.fif\n",
      "Closing /home/brain/Workspace/meg_driving/meg_preprocess/preprocessed/sub-xst/sensor_prep/preprocessed.fif\n",
      "[done]\n"
     ]
    }
   ],
   "source": [
    "raw_meg_data_file_name = \"cut10s_run1_tsss_mc.fif\"\n",
    "raw_meg_data_file_path_str = os.path.join(data_dir, subject_name, date_string)\n",
    "raw_filtered = mne.io.read_raw_fif(os.path.join(raw_meg_data_file_path_str, raw_meg_data_file_name), preload=True, allow_maxshield=True)\n",
    "\n",
    "if not os.path.isdir(os.path.join(preprocessed_subject_path_str,\"ICA_denoise\")):\n",
    "    os.mkdir(os.path.join(preprocessed_subject_path_str,\"ICA_denoise\"))\n",
    "if not os.path.isdir(os.path.join(preprocessed_subject_path_str,\"sensor_prep\")):\n",
    "    os.mkdir(os.path.join(preprocessed_subject_path_str,\"sensor_prep\"))\n",
    "\n",
    "# find EOG epochs\n",
    "try:\n",
    "    eog_evoked = mne.preprocessing.create_eog_epochs(raw_filtered).average()\n",
    "    eog_evoked.apply_baseline(baseline=(None, -0.2))\n",
    "    # eogplot = eog_evoked.plot_joint(show=False)\n",
    "    # for i in range(len(eogplot)):\n",
    "    #     eogplot[i].savefig(preprocessed_subject_path_str + \"/ICA_denoise/eog\" + str(i + 1) + \".tiff\")\n",
    "except:\n",
    "    print(\"RuntimeError: No EOG channel(s) found.\")\n",
    "\n",
    "# find ECG epochs\n",
    "try:\n",
    "    ecg_evoked = mne.preprocessing.create_ecg_epochs(raw_filtered).average()\n",
    "    ecg_evoked.apply_baseline(baseline=(None, -0.2))\n",
    "    # ecgplot = ecg_evoked.plot_joint(show=False)\n",
    "    # for i in range(len(ecgplot)):\n",
    "    #     ecgplot[i].savefig(preprocessed_subject_path_str + \"/ICA_denoise/ecg\" + str(i + 1) + \".tif\")\n",
    "except:\n",
    "    print(\"RuntimeError: No ECG channel(s) found.\")\n",
    "\n",
    "# ICA\n",
    "ica = mne.preprocessing.ICA(n_components=50, method=\"picard\", random_state=0)\n",
    "ica.fit(raw_filtered)\n",
    "icaplot = ica.plot_components(title=\"ICA components\", show=False)\n",
    "for i in range(len(icaplot)):\n",
    "    icaplot[i].savefig(preprocessed_subject_path_str + \"/ICA_denoise/ICAfig\" + str(i + 1) + \".tif\")\n",
    "\n",
    "eog_indices, eog_scores = ica.find_bads_eog(raw_filtered)\n",
    "ecg_indices, ecg_scores = ica.find_bads_ecg(raw_filtered)\n",
    "\n",
    "ica.exclude = []\n",
    "ica.exclude = eog_indices + ecg_indices\n",
    "# ica.exclude = ecg_indices\n",
    "raw_filtered = ica.apply(raw_filtered)\n",
    "\n",
    "# save file and records\n",
    "raw_filtered.save(os.path.join(preprocessed_subject_path_str,\"sensor_prep\", \"preprocessed.fif\"), overwrite=True)\n",
    "# with open(os.path.join(preprocessed_subject_path_str,\"03_flag.txt\"), \"w\") as f:\n",
    "#     f.write(\"Artefacts cleaned.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bem model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_subject_path_str = os.path.join(preprocessed_dir, subject_name)\n",
    "\n",
    "subject_freesurfer_dir=os.path.join(freesurfer_dir, subject_name)\n",
    "\n",
    "if not os.path.isdir(preprocessed_subject_path_str + \"/fig\"):\n",
    "    os.mkdir(preprocessed_subject_path_str + \"/fig\")\n",
    "if not os.path.isdir(preprocessed_subject_path_str + \"/src\"):\n",
    "    os.mkdir(preprocessed_subject_path_str + \"/src\")\n",
    "\n",
    "Brain = mne.viz.get_brain_class()\n",
    "brain = Brain(\n",
    "    subject=\"\",\n",
    "    views=\"lat\",\n",
    "    background=\"w\",\n",
    "    hemi=\"lh\",\n",
    "    surf=\"pial\",\n",
    "    subjects_dir=subject_freesurfer_dir,\n",
    "    title=\"Surface Reconstruction\",\n",
    "    size=(1000, 800),\n",
    ")\n",
    "brain.add_annotation(\"aparc.a2009s\", borders=False)\n",
    "brain.save_image(preprocessed_subject_path_str + \"/fig/recon.png\")\n",
    "brain.close()\n",
    "# construct the BEM\n",
    "\n",
    "info = mne.io.read_info(preprocessed_subject_path_str + \"/sensor_prep/preprocessed.fif\")\n",
    "conductivity = (0.3,)\n",
    "\n",
    "model = mne.make_bem_model(\n",
    "    \"\", ico=5, conductivity=conductivity, subjects_dir=subject_freesurfer_dir\n",
    ")\n",
    "bem_sol = mne.make_bem_solution(model)\n",
    "bem_model_fig = mne.viz.plot_bem(\n",
    "    subject=\"\", subjects_dir=subject_freesurfer_dir, orientation=\"coronal\", show=False\n",
    ")\n",
    "bem_model_fig.savefig(preprocessed_subject_path_str + \"/fig/bem_model_fig.tif\")\n",
    "mne.write_bem_solution(preprocessed_subject_path_str + \"/src/bem_sol.h5\", bem_sol, overwrite=True)\n",
    "\n",
    "# automatic coregistreation\n",
    "fiducials = \"estimated\"  # get fiducials from fsaverage\n",
    "coreg = mne.coreg.Coregistration(\n",
    "    info, subject=\"\", subjects_dir=subject_freesurfer_dir, fiducials=fiducials\n",
    ")\n",
    "plot_kwargs = dict(\n",
    "    subject=\"\",\n",
    "    subjects_dir=subject_freesurfer_dir,\n",
    "    surfaces=\"head-dense\",\n",
    "    dig=True,\n",
    "    eeg=[],\n",
    "    meg=\"sensors\",\n",
    "    show_axes=True,\n",
    "    coord_frame=\"meg\",\n",
    ")\n",
    "coreg.fit_fiducials(verbose=True)\n",
    "coreg.fit_icp(n_iterations=6, nasion_weight=2.0, verbose=True)\n",
    "coreg.omit_head_shape_points(distance=5.0 / 1000)\n",
    "coreg.fit_icp(n_iterations=20, nasion_weight=10.0, verbose=True)\n",
    "dists = coreg.compute_dig_mri_distances() * 1e3  # in mm\n",
    "print(\n",
    "    f\"Distance between HSP and MRI (mean/min/max):\\n{np.mean(dists):.2f} mm \"\n",
    "    f\"/ {np.min(dists):.2f} mm / {np.max(dists):.2f} mm\"\n",
    ")\n",
    "mne.write_trans(preprocessed_subject_path_str + \"/src/auto-trans.fif\", coreg.trans, overwrite=True)\n",
    "vol_src = mne.setup_volume_source_space(\n",
    "    \"\",\n",
    "    subjects_dir=subject_freesurfer_dir,\n",
    "    surface=subject_freesurfer_dir + \"/bem/inner_skull.surf\",\n",
    "    pos=4.0,\n",
    "    mindist=1.0,\n",
    ")\n",
    "mne.write_source_spaces(preprocessed_subject_path_str + \"/src/vol-src.fif\", vol_src, overwrite=True)\n",
    "surf_src = mne.setup_source_space(\n",
    "    \"\", subjects_dir=subject_freesurfer_dir, spacing=\"oct6\", add_dist=\"patch\"\n",
    ")\n",
    "mne.write_source_spaces(preprocessed_subject_path_str + \"/src/surf-src.fif\", surf_src, overwrite=True)\n",
    "# plot sources\n",
    "plot_bem_kwargs = dict(\n",
    "    subject=\"\",\n",
    "    subjects_dir=subject_freesurfer_dir,\n",
    "    brain_surfaces=\"white\",\n",
    "    orientation=\"coronal\",\n",
    "    slices=[50, 100, 150, 200],\n",
    ")\n",
    "src_fig = mne.viz.plot_bem(src=vol_src, **plot_bem_kwargs, show=False)\n",
    "src_fig.savefig(preprocessed_subject_path_str + \"/fig/vol_src.tif\")\n",
    "src_fig = mne.viz.plot_bem(src=surf_src, **plot_bem_kwargs, show=False)\n",
    "src_fig.savefig(preprocessed_subject_path_str + \"/fig/surf_src.tif\")\n",
    "\n",
    "# save records\n",
    "# with open(preprocessed_subject_path_str + \"/BEM_log.txt\", \"w\") as f:\n",
    "#     f.write(\"BEM constructed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### source localization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(preprocessed_subject_path_str + \"/fwd_model\"):\n",
    "    os.mkdir(preprocessed_subject_path_str + \"/fwd_model\")\n",
    "if not os.path.isdir(preprocessed_subject_path_str + \"/src\"):\n",
    "    os.mkdir(preprocessed_subject_path_str + \"/src\")\n",
    "\n",
    "# load MEG data and compute the evoked data\n",
    "vol_src = mne.read_source_spaces(preprocessed_subject_path_str + \"/src/vol-src.fif\")\n",
    "surf_src = mne.read_source_spaces(preprocessed_subject_path_str + \"/src/surf-src.fif\")\n",
    "trans = mne.read_trans(preprocessed_subject_path_str + \"/src/auto-trans.fif\")\n",
    "bem_sol = mne.read_bem_solution(preprocessed_subject_path_str + \"/src/bem_sol.h5\")\n",
    "data = mne.io.read_raw_fif(preprocessed_subject_path_str + \"/sensor_prep/preprocessed.fif\")\n",
    "\n",
    "# mne.set_eeg_reference(data, projection=True)\n",
    "cov = mne.compute_raw_covariance(\n",
    "    data, method=[\"shrunk\", \"empirical\"], rank=\"info\"\n",
    ")\n",
    "fwd_vol = mne.make_forward_solution(\n",
    "    preprocessed_subject_path_str + \"/sensor_prep/preprocessed.fif\",\n",
    "    trans=trans,\n",
    "    src=vol_src,\n",
    "    bem=bem_sol,\n",
    "    meg=True,\n",
    "    eeg=False,\n",
    "    n_jobs=20,\n",
    ")\n",
    "\n",
    "fwd_surf = mne.make_forward_solution(\n",
    "    preprocessed_subject_path_str + \"/sensor_prep/preprocessed.fif\",\n",
    "    trans=trans,\n",
    "    src=surf_src,\n",
    "    bem=bem_sol,\n",
    "    meg=True,\n",
    "    eeg=False,\n",
    "    mindist=3.0,\n",
    "    n_jobs=20,\n",
    ")\n",
    "\n",
    "inv_vol = mne.minimum_norm.make_inverse_operator(\n",
    "    data.info, fwd_vol, cov, loose=\"auto\", depth=0.8\n",
    ")\n",
    "inv_surf = mne.minimum_norm.make_inverse_operator(\n",
    "    data.info, fwd_surf, cov, loose=0.2, depth=0.8\n",
    ")\n",
    "\n",
    "snr = 3.0\n",
    "method = \"MNE\"\n",
    "lambda2 = 1.0 / snr**2\n",
    "\n",
    "# surface localisation\n",
    "# volume localisation\n",
    "# NOTE: volume files were separated into several sub-files to avoid memory error\n",
    "if not os.path.isdir(preprocessed_subject_path_str + \"/src/surf_file\"):\n",
    "    os.mkdir(preprocessed_subject_path_str + \"/src/surf_file\")\n",
    "if not os.path.isdir(preprocessed_subject_path_str + \"/src/vol_niifile\"):\n",
    "    os.mkdir(preprocessed_subject_path_str + \"/src/vol_niifile\")\n",
    "if not os.path.isdir(preprocessed_subject_path_str + \"/src/vol_stcfile\"):\n",
    "    os.mkdir(preprocessed_subject_path_str + \"/src/vol_stcfile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freesurfer_subject_path_dir = os.path.join(freesurfer_dir, subject_name)\n",
    "freesurfer_fsaverage_path_dir = os.path.join(freesurfer_dir, \"fsaverage\")\n",
    "\n",
    "subdata = data.copy()\n",
    "stc = mne.minimum_norm.apply_inverse_raw(\n",
    "    subdata,\n",
    "    inv_surf,\n",
    "    lambda2,\n",
    "    method,\n",
    ")\n",
    "if os.path.isdir(preprocessed_subject_path_str + \"/morph-maps\"):\n",
    "    shutil.rmtree(preprocessed_subject_path_str + \"/morph-maps\")\n",
    "morph = mne.compute_source_morph(\n",
    "    stc,\n",
    "    subject_from=freesurfer_subject_path_dir,\n",
    "    subject_to=freesurfer_fsaverage_path_dir,\n",
    "    src_to=surf_src,\n",
    "    subjects_dir=preprocessed_subject_path_str,\n",
    ")\n",
    "stc_fsaverage = morph.apply(stc)\n",
    "\n",
    "brain = stc_fsaverage.plot(\n",
    "    subjects_dir=freesurfer_subject_path_dir,\n",
    "    initial_time=0.1,\n",
    "    clim=dict(kind=\"value\", lims=[3, 6, 9]),\n",
    "    surface=\"flat\",\n",
    "    hemi=\"both\",\n",
    "    size=(1000, 500),\n",
    "    smoothing_steps=5,\n",
    "    time_viewer=False,\n",
    "    add_data_kwargs=dict(colorbar_kwargs=dict(label_font_size=10)),\n",
    ")\n",
    "\n",
    "# to help orient us, let's add a parcellation (red=auditory, green=motor,\n",
    "# blue=visual)\n",
    "brain.add_annotation(\"HCPMMP1_combined\", borders=2)\n",
    "\n",
    "# You can save a movie like the one on our documentation website with:\n",
    "# brain.save_movie(time_dilation=20, tmin=0.05, tmax=0.16,\n",
    "#                  interpolation='linear', framerate=10)\n",
    "\n",
    "stc_fsaverage.save(\n",
    "    preprocessed_subject_path_str\n",
    "    + \"/src/surf_file/surf-TS_src\",\n",
    "    ftype=\"stc\",\n",
    "    overwrite=True,\n",
    ")  # save a mne-file\n",
    "del stc  # release memory\n",
    "del stc_fsaverage\n",
    "\n",
    "stc = mne.minimum_norm.apply_inverse_raw(\n",
    "    subdata,\n",
    "    inv_vol,\n",
    "    lambda2,\n",
    "    method,\n",
    ")  # volume-source TC\n",
    "stc.save_as_volume(\n",
    "    preprocessed_subject_path_str\n",
    "    + \"/src/vol_niifile/vol-TS_src\"\n",
    "    + \".nii.gz\",\n",
    "    vol_src,\n",
    "    dest=\"mri\",\n",
    "    format=\"nifti1\",\n",
    "    overwrite=True,\n",
    ")  # save a nii-file\n",
    "stc.save(\n",
    "    preprocessed_subject_path_str\n",
    "    + \"/src/vol_stcfile/vol-TS_src_\",\n",
    "    ftype=\"stc\",\n",
    "    overwrite=True,\n",
    ")  # save a mne-file\n",
    "del subdata\n",
    "del stc\n",
    "\n",
    "if os.path.isdir(preprocessed_subject_path_str + \"/morph-maps\"):\n",
    "    shutil.rmtree(preprocessed_subject_path_str + \"/morph-maps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### roi_parc "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### surf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_estimate_lh_str = preprocessed_subject_path_str + \"/src/surf_file/surf-TS_src-lh.stc\"\n",
    "source_estimate_rh_str = preprocessed_subject_path_str + \"/src/surf_file/surf-TS_src-rh.stc\"\n",
    "\n",
    "mne.datasets.fetch_hcp_mmp_parcellation(subjects_dir=freesurfer_dir, accept=True)\n",
    "if not os.path.isdir(preprocessed_subject_path_str + \"/src/ROI_tc\"):\n",
    "    os.mkdir(preprocessed_subject_path_str + \"/src/ROI_tc\")\n",
    "\n",
    "surf_src = mne.read_source_spaces(preprocessed_subject_path_str + \"/src/surf-src.fif\")\n",
    "\n",
    "# left hemisphere\n",
    "stc_fsaverage_lh = mne.read_source_estimate(source_estimate_lh_str)\n",
    "stc_fsaverage_lh.subject = \"\"\n",
    "mne.datasets.fetch_hcp_mmp_parcellation(subjects_dir=freesurfer_dir, accept=True)\n",
    "hcp_mmp_lh = mne.read_labels_from_annot(\n",
    "    subject=\"\",\n",
    "    parc=\"lh.HCPMMP1\",\n",
    "    hemi=\"lh\",\n",
    "    annot_fname=os.path.join(freesurfer_dir, \"fsaverage\") + \"/label/lh.HCPMMP1.annot\",\n",
    "    subjects_dir=os.path.join(freesurfer_dir, \"fsaverage\"),\n",
    ")\n",
    "ts_lh = mne.extract_label_time_course(\n",
    "    stcs=stc_fsaverage_lh, labels=hcp_mmp_lh[1:181], src=surf_src\n",
    ")\n",
    "scipy.io.savemat(\n",
    "    preprocessed_subject_path_str\n",
    "    + \"/src/ROI_tc/ts_HCPmmp_lh_\"\n",
    "    + subject_name\n",
    "    + \".mat\",\n",
    "    {\"ts_lh\": ts_lh},\n",
    ")\n",
    "with open(\n",
    "    preprocessed_subject_path_str\n",
    "    + \"/src/ROI_tc/lh_ROI_label_\"\n",
    "    + subject_name\n",
    "    + \".csv\",\n",
    "    \"w\",\n",
    "    newline=\"\",\n",
    ") as csvfile:\n",
    "    ROIwriter = csv.writer(\n",
    "        csvfile, delimiter=\" \", quotechar=\"|\", quoting=csv.QUOTE_MINIMAL\n",
    "    )\n",
    "    for i_ROI in range(1, 181):\n",
    "        ROIwriter.writerow([hcp_mmp_lh[i_ROI]])\n",
    "\n",
    "# right hemisphere\n",
    "stc_fsaverage_rh = mne.read_source_estimate(source_estimate_rh_str)\n",
    "stc_fsaverage_rh.subject = \"\"\n",
    "hcp_mmp_rh = mne.read_labels_from_annot(\n",
    "    subject=\"\",\n",
    "    parc=\"rh.HCPMMP1\",\n",
    "    hemi=\"rh\",\n",
    "    annot_fname=os.path.join(freesurfer_dir, \"fsaverage\") + \"/label/rh.HCPMMP1.annot\",\n",
    "    subjects_dir=os.path.join(freesurfer_dir, \"fsaverage\"),\n",
    ")\n",
    "ts_rh = mne.extract_label_time_course(\n",
    "    stcs=stc_fsaverage_rh, labels=hcp_mmp_rh[1:181], src=surf_src\n",
    ")\n",
    "scipy.io.savemat(\n",
    "    preprocessed_subject_path_str\n",
    "    + \"/src/ROI_tc/ts_HCPmmp_rh_\"\n",
    "    + subject_name\n",
    "    + \".mat\",\n",
    "    {\"ts_rh\": ts_rh},\n",
    ")\n",
    "with open(\n",
    "    preprocessed_subject_path_str\n",
    "    + \"/src/ROI_tc/rh_ROI_label_\"\n",
    "    + subject_name\n",
    "    + \".csv\",\n",
    "    \"w\",\n",
    "    newline=\"\",\n",
    ") as csvfile:\n",
    "    ROIwriter = csv.writer(\n",
    "        csvfile, delimiter=\" \", quotechar=\"|\", quoting=csv.QUOTE_MINIMAL\n",
    "    )\n",
    "    for i_ROI in range(1, 181):\n",
    "        ROIwriter.writerow([hcp_mmp_rh[i_ROI]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### vol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_src = mne.read_source_spaces(preprocessed_subject_path_str + \"/src/vol-src.fif\")\n",
    "source_estimate_vol_str = preprocessed_subject_path_str + \"/src/vol_stcfile/vol-TS_src_-vl.stc\"\n",
    "stc_vol= mne.read_source_estimate(source_estimate_vol_str)\n",
    "\n",
    "print(\"check source vol...\")\n",
    "print(\"vol_src:\", vol_src, '\\n num:', 39 * 46 * 40)\n",
    "print(\"stc_vol:\", stc_vol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "\n",
    "matdic = loadmat(preprocessed_subject_path_str + \"/src/ROI_tc/\" + \"ts_HCPmmp_lh_\"+ subject_name+\".mat\")\n",
    "\n",
    "ts_lh = np.squeeze(matdic[\"ts_lh\"])\n",
    "print(\"ts_lh shape =\", ts_lh.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mask the nii data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load atlas brain regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# read design matrix\n",
    "atlas_labels_txt = \"Atlas MNI_Glasser_HCP_v1.0, 360 regions.txt\"\n",
    "atlas_labels_file = os.path.join(project_dir, \"masks\", atlas_labels_txt)\n",
    "\n",
    "in_file = open(atlas_labels_file,'r')\n",
    "brain_region_dict={}\n",
    "\n",
    "# distinguish left and right brain regions\n",
    "atlas_all_lines = in_file.readlines()\n",
    "\n",
    "for i_line in range(0,len(atlas_all_lines)):\n",
    "    if \"u:L_\" in atlas_all_lines[i_line]:\n",
    "        \n",
    "        atlas_brain_region_label = []\n",
    "\n",
    "        # left brain regions\n",
    "        atlas_line_left = atlas_all_lines[i_line]\n",
    "        atlas_temp = atlas_line_left.split(\":\")\n",
    "        atlas_brain_region_name = atlas_temp[1][2:]\n",
    "        # print(atlas_brain_region_name)\n",
    "        atlas_brain_region_label_left = int(atlas_temp[2][:-1])\n",
    "        atlas_brain_region_label.append(atlas_brain_region_label_left)\n",
    "        \n",
    "        # right brain regions\n",
    "        atlas_line_right = atlas_all_lines[i_line+1]\n",
    "        atlas_temp = atlas_line_right.split(\":\")\n",
    "        atlas_brain_region_name = atlas_temp[1][2:]\n",
    "        atlas_brain_region_label_right = int(atlas_temp[2][:-1])\n",
    "        atlas_brain_region_label.append(atlas_brain_region_label_right)\n",
    "\n",
    "        # put into dict\n",
    "        brain_region_dict[atlas_brain_region_name] = atlas_brain_region_label\n",
    "\n",
    "print(len(brain_region_dict),brain_region_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain_region_name_list = list(brain_region_dict.keys())\n",
    "brain_region_label_list = list(brain_region_dict.values())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load hcp mni atlas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the mask image\n",
    "mni_glasser_hcp_atlas_name = \"MNI_Glasser_HCP_v1.0.nii.gz\"\n",
    "mni_glasser_hcp_atlas_file = os.path.join(project_dir, \"masks\", mni_glasser_hcp_atlas_name)\n",
    "\n",
    "mni_glasser_hcp_atlas_mask = nib.load(mni_glasser_hcp_atlas_file)\n",
    "mni_glasser_hcp_atlas_mask_data = mni_glasser_hcp_atlas_mask.get_fdata()\n",
    "print(\"mni_glasser_hcp_atlas_mask_data.shape:\", mni_glasser_hcp_atlas_mask_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load nii data\n",
    "current_file_str = preprocessed_subject_path_str + \"/src/vol_niifile/vol-TS_src.nii.gz\"\n",
    "nii_data = nib.load(current_file_str)\n",
    "nii_data = nil.image.index_img(nii_data, slice(0, 1))\n",
    "\n",
    "saved_mask_name = \"vol_ts_src.nii.gz\"\n",
    "mask_path = os.path.join(project_dir, \"masks\", saved_mask_name)\n",
    "nib.save(nii_data, mask_path)\n",
    "print(\"nii_data.shape:\", nii_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### resample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash \n",
    "#!/bin/bash\n",
    "export PATH=$PATH:/home/brain/abin\n",
    "\n",
    "project_dir=\"/home/brain/Workspace/meg_driving/preprocess\"\n",
    "\n",
    "3dfractionize -template ${project_dir}/masks/vol_ts_src.nii.gz -input ${project_dir}/masks/MNI_Glasser_HCP_v1.0.nii.gz -clip 0.0 -preserve -prefix ${project_dir}/masks/resampled_mni_mask.nii.gz -overwrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the mask image\n",
    "resampled_mni_mask_name = \"resampled_mni_mask.nii.gz\"\n",
    "resampled_mni_mask_file = os.path.join(project_dir, \"masks\", resampled_mni_mask_name)\n",
    "\n",
    "resampled_mni_mask = nib.load(resampled_mni_mask_file)\n",
    "resampled_mni_mask_data = resampled_mni_mask.get_fdata()\n",
    "print(\"resampled_mni_mask_data.shape:\", resampled_mni_mask_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create region-based masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_brain_region_name in np.arange(0, len(brain_region_name_list)):\n",
    "  \n",
    "    brain_region_name = brain_region_name_list[i_brain_region_name]\n",
    "\n",
    "    # mask_name = \"resampled_MNI_Glasser_HCP_neural_mask.nii.gz\"\n",
    "    mask_name = \"resampled_mni_mask.nii.gz\"\n",
    "    # mask_name = \"MNI_Glasser_HCP_v1.0.nii.gz\"\n",
    "    \n",
    "    mask_file = os.path.join(project_dir, \"masks\", mask_name)\n",
    "\n",
    "    # Load the mask image\n",
    "    mask = nib.load(mask_file)\n",
    "    mask_data = mask.get_fdata()\n",
    "\n",
    "    if i_brain_region_name != 0: # remove the effect of first region V1, id V1 is same as mask 0\n",
    "      mask_data[mask_data==1] = 0\n",
    "\n",
    "    for i_num_region in brain_region_label_list[i_brain_region_name]:\n",
    "        mask_data[mask_data==i_num_region] = 1\n",
    "\n",
    "    mask_data[mask_data!=1] = 0\n",
    "\n",
    "    voxel_num = np.count_nonzero(mask_data[mask_data!=0])\n",
    "    print(brain_region_name, voxel_num)\n",
    "\n",
    "    mask_nifti = nib.Nifti1Image(mask_data, mask.affine, mask.header)\n",
    "\n",
    "    saved_mask_name = brain_region_name+\"_resampled_mni_mask.nii.gz\"\n",
    "\n",
    "    mask_path = os.path.join(project_dir, \"masks\" , \"region_based_masks\", saved_mask_name)\n",
    "\n",
    "    nib.save(mask_nifti, mask_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mask the nii data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain_region_nii_data_list = []\n",
    "\n",
    "for brain_region_name in brain_region_name_list:\n",
    "\n",
    "    print('brain_region_name:',brain_region_name)\n",
    "\n",
    "    ### load mask\n",
    "    mask_name = brain_region_name+\"_resampled_mni_mask.nii.gz\"\n",
    "    mask_file = os.path.join(project_dir, \"masks\", \"region_based_masks\", mask_name)\n",
    "\n",
    "    # Load the mask image\n",
    "    mask = nib.load(mask_file)\n",
    "\n",
    "    mask_data = mask.get_fdata()\n",
    "\n",
    "    voxel_num = np.count_nonzero(mask_data[mask_data!=0])\n",
    "\n",
    "    print(voxel_num)\n",
    "    if voxel_num != 0: \n",
    "\n",
    "        sub_dir = os.path.join(preprocessed_dir, subject_name)\n",
    "\n",
    "        # load nii data\n",
    "        current_file_str = preprocessed_subject_path_str + \"/src/vol_niifile/vol-TS_src.nii.gz\"\n",
    "        nii_data = nib.load(current_file_str)\n",
    "\n",
    "        # mask bold signal\n",
    "        nifti_masker = nil.input_data.NiftiMasker(mask_img=mask)\n",
    "        masked_data = nifti_masker.fit_transform(nii_data)\n",
    "        masked_data = masked_data.T\n",
    "\n",
    "        # do it for epi data\n",
    "        masked_data = stats.zscore(masked_data, axis=1, ddof=1)\n",
    "        masked_data = np.nan_to_num(masked_data)\n",
    "        # print(\"masked_data shape (voxel_num,TR_num)\", masked_data.shape)\n",
    "\n",
    "        # add a brain region\n",
    "        brain_region_nii_data_list.append(masked_data)\n",
    "\n",
    "    elif voxel_num == 0:\n",
    "\n",
    "        masked_data = np.empty((0,0), float)\n",
    "\n",
    "        brain_region_nii_data_list.append(masked_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"num of brain regions:\", len(brain_region_nii_data_list))\n",
    "\n",
    "for i in range(0, len(brain_region_name_list)):\n",
    "    print(brain_region_name_list[i],':',brain_region_nii_data_list[i].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### save and load brain_region_nii_data_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdf5storage\n",
    "\n",
    "matdic = {\"brain_region_name_list\":brain_region_name_list,\n",
    "          \"brain_region_nii_data_list\":brain_region_nii_data_list}\n",
    "file_dir = os.path.join(preprocessed_dir, subject_name)\n",
    "hdf5storage.savemat(file_dir+\"/brain_region_nii_data_list.mat\", matdic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdf5storage\n",
    "\n",
    "file_dir = os.path.join(preprocessed_dir, subject_name)\n",
    "matdic = hdf5storage.loadmat(file_dir+\"/brain_region_nii_data_list.mat\", )\n",
    "\n",
    "brain_region_nii_data_list = matdic[\"brain_region_nii_data_list\"]\n",
    "brain_region_name_list = matdic[\"brain_region_name_list\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"num of brain regions:\", len(brain_region_nii_data_list))\n",
    "\n",
    "for i in range(0, len(brain_region_name_list)):\n",
    "    print(brain_region_name_list[i],':',brain_region_nii_data_list[i].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check the final results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check average of each region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_src = mne.read_source_spaces(preprocessed_subject_path_str + \"/src/vol-src.fif\")\n",
    "source_estimate_vol_str = preprocessed_subject_path_str + \"/src/vol_stcfile/vol-TS_src_-vl.stc\"\n",
    "stc_vol= mne.read_source_estimate(source_estimate_vol_str)\n",
    "\n",
    "print(\"check source vol...\")\n",
    "print(\"vol_src:\", vol_src, '\\n num:', 39 * 46 * 40)\n",
    "print(\"stc_vol:\", stc_vol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "\n",
    "matdic = loadmat(preprocessed_subject_path_str + \"/src/ROI_tc/\" + \"ts_HCPmmp_lh_sub-test.mat\")\n",
    "\n",
    "ts_lh = np.squeeze(matdic[\"ts_lh\"])\n",
    "print(\"ts_lh shape =\", ts_lh.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check each region with mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdf5storage\n",
    "\n",
    "file_dir = os.path.join(preprocessed_dir, subject_name)\n",
    "matdic = hdf5storage.loadmat(file_dir+\"/brain_region_nii_data_list.mat\", )\n",
    "\n",
    "brain_region_nii_data_list = matdic[\"brain_region_nii_data_list\"]\n",
    "brain_region_name_list = matdic[\"brain_region_name_list\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"num of brain regions:\", len(brain_region_nii_data_list))\n",
    "\n",
    "for i in range(0, len(brain_region_name_list)):\n",
    "    print(brain_region_name_list[i],':',brain_region_nii_data_list[i].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
